{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8035/20272918.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdonut\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcomplete_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstandardize_kpi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDonut\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDonutTrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDonutPredictor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtfsnippet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from donut import complete_timestamp, standardize_kpi, Donut, DonutTrainer, DonutPredictor\n",
    "from tensorflow import keras as K\n",
    "from tfsnippet.modules import Sequential\n",
    "from tfsnippet.utils import get_variables_as_dict, VariableSaver\n",
    "import tensorflow.compat.v1 as tf\n",
    "import os\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import f1_score\n",
    "tf.disable_v2_behavior()\n",
    " \n",
    "# path to the dataset\n",
    "file_csv = \"cpu4.csv\"\n",
    " \n",
    "# Read the raw data.\n",
    "data = pd.read_csv(file_csv)\n",
    "timestamp = data[\"timestamp\"]\n",
    "values = data[\"value\"]\n",
    "labels = data[\"label\"]\n",
    "dataset_name = file_csv.split('.')[0]\n",
    "print(\"Timestamps: {}\".format(timestamp.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Collecting pandas\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/3e/0c/23764c4635dcb0a784a787498d56847b90ebf974e65f4ab4053a5d97b1a5/pandas-1.3.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.3 MB 553 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pytz>=2017.3 in /root/miniconda3/lib/python3.7/site-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: numpy>=1.17.3; platform_machine != \"aarch64\" and platform_machine != \"arm64\" and python_version < \"3.10\" in /root/miniconda3/lib/python3.7/site-packages (from pandas) (1.18.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /root/miniconda3/lib/python3.7/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /root/miniconda3/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Installing collected packages: pandas\n",
      "Successfully installed pandas-1.3.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Collecting sklearn\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/1e/7a/dbb3be0ce9bd5c8b7e3d87328e79063f8b263b2b1bfa4774cb1147bfcd3f/sklearn-0.0.tar.gz (1.1 kB)\n",
      "Collecting scikit-learn\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/bd/05/e561bc99a615b5c099c7a9355409e5e57c525a108f1c2e156abb005b90a6/scikit_learn-1.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (24.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 24.8 MB 623 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting joblib>=0.11\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/3e/d5/0163eb0cfa0b673aa4fe1cd3ea9d8a81ea0f32e50807b0c295871e4aab2e/joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
      "\u001b[K     |████████████████████████████████| 306 kB 657 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.14.6 in /root/miniconda3/lib/python3.7/site-packages (from scikit-learn->sklearn) (1.18.5)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/61/cf/6e354304bcb9c6413c4e02a747b600061c21d38ba51e7e544ac7bc66aecc/threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Collecting scipy>=1.1.0\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/58/4f/11f34cfc57ead25752a7992b069c36f5d18421958ebd6466ecd849aeaf86/scipy-1.7.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 38.1 MB 651 kB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: sklearn\n",
      "  Building wheel for sklearn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1316 sha256=806fb46565046659de59a3fb0900b1867574dad711f9a9fff2d8b28055b3eb86\n",
      "  Stored in directory: /root/.cache/pip/wheels/23/51/c2/5159d7bbfa5e6ba511213e0317a65331f1174d1c155af26def\n",
      "Successfully built sklearn\n",
      "Installing collected packages: joblib, threadpoolctl, scipy, scikit-learn, sklearn\n",
      "Successfully installed joblib-1.1.0 scikit-learn-1.0.2 scipy-1.7.3 sklearn-0.0 threadpoolctl-3.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /root/miniconda3/lib/python3.7/site-packages/tfsnippet/utils/scope.py:114: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/miniconda3/lib/python3.7/site-packages/tfsnippet/mathops/_tfops.py:20: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/miniconda3/lib/python3.7/site-packages/tfsnippet/mathops/_tfops.py:21: The name tf.log1p is deprecated. Please use tf.math.log1p instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/autodl-tmp/donut/training.py:78: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/compat/v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "Timestamps: 17568\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from donut import complete_timestamp, standardize_kpi, Donut, DonutTrainer, DonutPredictor\n",
    "from tensorflow import keras as K\n",
    "from tfsnippet.modules import Sequential\n",
    "from tfsnippet.utils import get_variables_as_dict, VariableSaver\n",
    "import tensorflow.compat.v1 as tf\n",
    "import os\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import f1_score\n",
    "tf.disable_v2_behavior()\n",
    " \n",
    "# path to the dataset\n",
    "file_csv = \"cpu4.csv\"\n",
    " \n",
    "# Read the raw data.\n",
    "data = pd.read_csv(file_csv)\n",
    "timestamp = data[\"timestamp\"]\n",
    "values = data[\"value\"]\n",
    "labels = data[\"label\"]\n",
    "dataset_name = file_csv.split('.')[0]\n",
    "print(\"Timestamps: {}\".format(timestamp.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing points: 0\n",
      "Labeled anomalies: 209\n"
     ]
    }
   ],
   "source": [
    "# Complete the timestamp filling missing points with zeros, and obtain the missing point indicators.\n",
    "# 该complete_timestamp函数用 0 填充缺失点，并返回一个数组，指示它们在增强数据集中的位置。\n",
    "timestamp, missing, (values, labels) = complete_timestamp(timestamp, (values, labels))\n",
    "print(\"Missing points: {}\".format(np.sum(missing == 1)))\n",
    "print(\"Labeled anomalies: {}\".format(np.sum(labels == 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in test set: 5270\n",
      "Anomalies in test set: 14\n"
     ]
    }
   ],
   "source": [
    "# Split the training and testing data.\n",
    "# 分离为训练集和测试集\n",
    "test_portion = 0.3\n",
    "test_n = int(len(values) * test_portion)\n",
    "train_values, test_values = values[:-test_n], values[-test_n:]\n",
    "train_labels, test_labels = labels[:-test_n], labels[-test_n:]\n",
    "train_missing, test_missing = missing[:-test_n], missing[-test_n:]\n",
    "print(\"Rows in test set: {}\".format(test_values.shape[0]))\n",
    "print(\"Anomalies in test set: {}\".format(np.sum(test_labels == 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train values mean: 0.008981638588011265\n",
      "Train values std: 0.9900397062301636\n"
     ]
    }
   ],
   "source": [
    "# Standardize the training and testing data, anomaly points or missing points are excluded\n",
    "# 对训练集和测试集进行标准化，从均值和标准差的计算中排除缺失点或异常。\n",
    "train_values, mean, std = standardize_kpi(\n",
    "    train_values, excludes=np.logical_or(train_labels, train_missing))\n",
    "test_values, _, _ = standardize_kpi(test_values, mean=mean, std=std)\n",
    "print(\"Train values mean: {}\".format(mean))\n",
    "print(\"Train values std: {}\".format(std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /root/miniconda3/lib/python3.7/site-packages/tfsnippet/utils/scope.py:95: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/miniconda3/lib/python3.7/site-packages/tfsnippet/utils/scope.py:26: The name tf.VariableScope is deprecated. Please use tf.compat.v1.VariableScope instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/autodl-tmp/donut/training.py:110: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/autodl-tmp/donut/training.py:116: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/miniconda3/lib/python3.7/site-packages/tfsnippet/utils/reuse.py:56: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /root/autodl-tmp/donut/model.py:26: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From /root/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From /root/miniconda3/lib/python3.7/site-packages/zhusuan/distributions/univariate.py:167: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/autodl-tmp/donut/training.py:128: The name tf.losses.get_regularization_loss is deprecated. Please use tf.compat.v1.losses.get_regularization_loss instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/miniconda3/lib/python3.7/site-packages/tfsnippet/utils/session.py:106: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/clip_ops.py:172: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /root/autodl-tmp/donut/training.py:154: The name tf.check_numerics is deprecated. Please use tf.debugging.check_numerics instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/autodl-tmp/donut/training.py:168: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/autodl-tmp/donut/training.py:170: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/autodl-tmp/donut/training.py:174: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sliding_window = 120\n",
    " \n",
    "# define the model inside the 'model_vs' scope\n",
    "# 在“model_vs”的范围内定义构建它的模型\n",
    "with tf.variable_scope('model') as model_vs:\n",
    "    model = Donut(\n",
    "        h_for_p_x=Sequential([\n",
    "            K.layers.Dense(100, kernel_regularizer=K.regularizers.l2(0.001),\n",
    "                           activation=tf.nn.relu),\n",
    "            K.layers.Dense(100, kernel_regularizer=K.regularizers.l2(0.001),\n",
    "                           activation=tf.nn.relu),\n",
    "        ]),\n",
    "        h_for_q_z=Sequential([\n",
    "            K.layers.Dense(100, kernel_regularizer=K.regularizers.l2(0.001),\n",
    "                           activation=tf.nn.relu),\n",
    "            K.layers.Dense(100, kernel_regularizer=K.regularizers.l2(0.001),\n",
    "                           activation=tf.nn.relu),\n",
    "        ]),\n",
    "        x_dims=sliding_window,\n",
    "        z_dims=5,\n",
    "    )\n",
    "# use DonutTrainer class to train the model\n",
    "# 该类DonutTrainer用于训练模型\n",
    "trainer = DonutTrainer(model=model, model_vs=model_vs, max_epoch=30)\n",
    "# use DonutPredictor class to make predictions\n",
    "# 同时通过该类进行预测DonutPredictor\n",
    "predictor = DonutPredictor(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /root/miniconda3/lib/python3.7/site-packages/tfsnippet/utils/session.py:71: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/miniconda3/lib/python3.7/site-packages/tfsnippet/utils/session.py:222: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-21 18:38:44.454036: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\n",
      "2022-05-21 18:38:44.462682: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2500000000 Hz\n",
      "2022-05-21 18:38:44.466292: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5618fee4b490 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2022-05-21 18:38:44.466312: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /root/miniconda3/lib/python3.7/site-packages/tfsnippet/utils/session.py:153: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n",
      "Trainable Parameters                     (58,150 in total)\n",
      "----------------------------------------------------------\n",
      "donut/p_x_given_z/x_mean/bias           (120,)         120\n",
      "donut/p_x_given_z/x_mean/kernel         (100, 120)  12,000\n",
      "donut/p_x_given_z/x_std/bias            (120,)         120\n",
      "donut/p_x_given_z/x_std/kernel          (100, 120)  12,000\n",
      "donut/q_z_given_x/z_mean/bias           (5,)             5\n",
      "donut/q_z_given_x/z_mean/kernel         (100, 5)       500\n",
      "donut/q_z_given_x/z_std/bias            (5,)             5\n",
      "donut/q_z_given_x/z_std/kernel          (100, 5)       500\n",
      "sequential/forward/_0/dense/bias        (100,)         100\n",
      "sequential/forward/_0/dense/kernel      (5, 100)       500\n",
      "sequential/forward/_1/dense_1/bias      (100,)         100\n",
      "sequential/forward/_1/dense_1/kernel    (100, 100)  10,000\n",
      "sequential_1/forward/_0/dense_2/bias    (100,)         100\n",
      "sequential_1/forward/_0/dense_2/kernel  (120, 100)  12,000\n",
      "sequential_1/forward/_1/dense_3/bias    (100,)         100\n",
      "sequential_1/forward/_1/dense_3/kernel  (100, 100)  10,000\n",
      "\n",
      "[Epoch 4/30, Step 100, ETA 8.977s] step time: 0.009386s (±0.03423s); valid time: 0.145s; loss: 84.242 (±55.094); valid loss: 10.9943 (*)\n",
      "[Epoch 7/30, Step 200, ETA 6.322s] step time: 0.005411s (±0.007812s); valid time: 0.0661s; loss: -12.8857 (±8.24003); valid loss: -20.1317 (*)\n",
      "WARNING:tensorflow:From /root/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "[Epoch 10/30, Step 300, ETA 5.028s] step time: 0.005345s (±0.007829s); valid time: 0.06922s; loss: -28.8294 (±4.32704); valid loss: -33.1946 (*)\n",
      "[Epoch 10/30, Step 330, ETA 4.65s] Learning rate decreased to 0.00075\n",
      "[Epoch 13/30, Step 400, ETA 4.125s] step time: 0.005593s (±0.008109s); valid time: 0.06889s; loss: -40.2843 (±3.43907); valid loss: -43.0759 (*)\n",
      "[Epoch 16/30, Step 500, ETA 3.323s] step time: 0.005427s (±0.007823s); valid time: 0.06758s; loss: -47.039 (±2.55631); valid loss: -46.5456 (*)\n",
      "[Epoch 19/30, Step 600, ETA 2.616s] step time: 0.005846s (±0.01121s); valid time: 0.1044s; loss: -52.3257 (±2.41147); valid loss: -50.4693 (*)\n",
      "[Epoch 20/30, Step 660, ETA 2.161s] Learning rate decreased to 0.0005625000000000001\n",
      "[Epoch 22/30, Step 700, ETA 1.912s] step time: 0.005384s (±0.007841s); valid time: 0.06791s; loss: -55.5487 (±2.25676); valid loss: -52.9781 (*)\n",
      "[Epoch 25/30, Step 800, ETA 1.238s] step time: 0.005457s (±0.007753s); valid time: 0.06636s; loss: -58.7939 (±1.73445); valid loss: -53.915 (*)\n",
      "[Epoch 28/30, Step 900, ETA 0.5808s] step time: 0.005454s (±0.007656s); valid time: 0.06492s; loss: -60.4085 (±2.17226); valid loss: -55.8858 (*)\n",
      "[Epoch 30/30, Step 990, ETA 0s] Learning rate decreased to 0.00042187500000000005\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpe3fwunxv/variables.dat-900\n"
     ]
    }
   ],
   "source": [
    "# 将对其进行训练并创建一个文件来保存它，以便我们将来可以重用相同的模型来进行新的预测，从而避免再次保留它。\n",
    "save_dir = \"model1/\" + dataset_name + \"/\"\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "saved = True\n",
    "if len(os.listdir(save_dir)) == 0:\n",
    "    saved = False\n",
    " \n",
    "if saved is False:\n",
    "    with tf.Session().as_default():\n",
    "        # train the model\n",
    "        trainer.fit(train_values, train_labels, train_missing, mean, std)\n",
    "        # save variables to 'save_dir' directory\n",
    "        var_dict = get_variables_as_dict(model_vs)\n",
    "        saver = VariableSaver(var_dict, save_dir)\n",
    "        saver.save()\n",
    "        saved = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /root/autodl-tmp/model1/cpu4/variables.dat\n",
      "WARNING:tensorflow:From /root/autodl-tmp/donut/reconstruction.py:53: The name tf.assert_equal is deprecated. Please use tf.compat.v1.assert_equal instead.\n",
      "\n",
      "Number of predictions: 5151\n",
      "--  final results --\n",
      "Best anomaly threshold 6.800000000000002\n",
      "Anomalies found: 10/14\n",
      "--- anomaly rows ---\n",
      "precision: 1.000\n",
      "recall: 0.714\n",
      "fscore: 0.833\n"
     ]
    }
   ],
   "source": [
    "# 模型经过训练 就可以在预测阶段结束代码\n",
    "if saved:\n",
    "    with tf.Session().as_default():\n",
    "        # restore variables from 'save_dir'\n",
    "        # 在\"save_dir\"中储存变量\n",
    "        saver = VariableSaver(get_variables_as_dict(model_vs), save_dir)\n",
    "        saver.restore()\n",
    "        # make predictions 做出检测\n",
    "        test_score = predictor.get_score(test_values, test_missing)\n",
    "        print(\"Number of predictions: {}\".format(test_score.shape[0]))\n",
    "        # try different thresholds\n",
    "        # 使用不同的阈值\n",
    "        best_threshold = 0\n",
    "        best_f1 = 0\n",
    "        best_predictions = []\n",
    "        thresholds = np.arange(5, 50, 0.2)\n",
    "        for t in thresholds:\n",
    "            threshold = t  # can be changed to better fit the training data\n",
    "            anomaly_predictions = []\n",
    "            for l in test_score:\n",
    "                if abs(l) > threshold:\n",
    "                    anomaly_predictions.append(1)\n",
    "                else:\n",
    "                    anomaly_predictions.append(0)\n",
    "            # strategy to compute modified metrics\n",
    "            # 计算修改后的指标的策略\n",
    "            \n",
    "            for i in range(sliding_window-1, len(anomaly_predictions)):\n",
    "                if anomaly_predictions[i-sliding_window+1] == 1 and test_labels[i] == 1:  # true positive\n",
    "                    j = i-1\n",
    "                    while j >= sliding_window-1 and test_labels[j] == 1\\\n",
    "                            and anomaly_predictions[j-sliding_window+1] == 0:\n",
    "                        anomaly_predictions[j-sliding_window+1] = 1\n",
    "                        j -= 1\n",
    "                    j = i+1\n",
    "                    while j < len(anomaly_predictions) and test_labels[j] == 1\\\n",
    "                            and anomaly_predictions[j-sliding_window+1] == 0:\n",
    "                        anomaly_predictions[j-sliding_window+1] = 1\n",
    "                        j += 1\n",
    "            f1 = f1_score(test_labels[sliding_window-1:], anomaly_predictions, average='binary')\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_threshold = threshold\n",
    "                best_predictions = anomaly_predictions\n",
    " \n",
    "        anomaly_predictions = np.array(best_predictions)\n",
    "        print(\"--  final results --\")\n",
    "        print(\"Best anomaly threshold {}\".format(best_threshold))\n",
    "        print(\"Anomalies found: {}/{}\".format(np.sum(anomaly_predictions == 1), np.sum(test_labels == 1)))\n",
    "        prfs = precision_recall_fscore_support(test_labels[sliding_window-1:], anomaly_predictions)\n",
    "        print(\"--- anomaly rows ---\")\n",
    "        print(\"precision: {:.3f}\".format(prfs[0][1]))\n",
    "        print(\"recall: {:.3f}\".format(prfs[1][1]))\n",
    "        print(\"fscore: {:.3f}\".format(prfs[2][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
