{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03260ca9-09cc-4424-97dd-236d47f59d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamps: 99999\n",
      "Missing points: 3037\n",
      "Labeled anomalies: 9168\n",
      "Rows in test set: 30910\n",
      "Anomalies in test set: 1023\n",
      "Train values mean: 1.9223467111587524\n",
      "Train values std: 0.6066468954086304\n",
      "WARNING:tensorflow:From /root/miniconda3/lib/python3.7/site-packages/tfsnippet/utils/scope.py:95: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/miniconda3/lib/python3.7/site-packages/tfsnippet/utils/scope.py:26: The name tf.VariableScope is deprecated. Please use tf.compat.v1.VariableScope instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/autodl-tmp/donut/training.py:110: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/autodl-tmp/donut/training.py:116: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/miniconda3/lib/python3.7/site-packages/tfsnippet/utils/reuse.py:56: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /root/autodl-tmp/donut/model.py:26: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From /root/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From /root/miniconda3/lib/python3.7/site-packages/zhusuan/distributions/univariate.py:167: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/autodl-tmp/donut/training.py:128: The name tf.losses.get_regularization_loss is deprecated. Please use tf.compat.v1.losses.get_regularization_loss instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/miniconda3/lib/python3.7/site-packages/tfsnippet/utils/session.py:106: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/clip_ops.py:172: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /root/autodl-tmp/donut/training.py:154: The name tf.check_numerics is deprecated. Please use tf.debugging.check_numerics instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/autodl-tmp/donut/training.py:168: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/autodl-tmp/donut/training.py:170: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/autodl-tmp/donut/training.py:174: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/miniconda3/lib/python3.7/site-packages/tfsnippet/utils/session.py:71: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/miniconda3/lib/python3.7/site-packages/tfsnippet/utils/session.py:222: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/miniconda3/lib/python3.7/site-packages/tfsnippet/utils/session.py:153: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-20 21:24:11.958513: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\n",
      "2022-05-20 21:24:11.966840: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2500000000 Hz\n",
      "2022-05-20 21:24:11.970455: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55b89c192040 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2022-05-20 21:24:11.970474: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable Parameters                     (58,150 in total)\n",
      "----------------------------------------------------------\n",
      "donut/p_x_given_z/x_mean/bias           (120,)         120\n",
      "donut/p_x_given_z/x_mean/kernel         (100, 120)  12,000\n",
      "donut/p_x_given_z/x_std/bias            (120,)         120\n",
      "donut/p_x_given_z/x_std/kernel          (100, 120)  12,000\n",
      "donut/q_z_given_x/z_mean/bias           (5,)             5\n",
      "donut/q_z_given_x/z_mean/kernel         (100, 5)       500\n",
      "donut/q_z_given_x/z_std/bias            (5,)             5\n",
      "donut/q_z_given_x/z_std/kernel          (100, 5)       500\n",
      "sequential/forward/_0/dense/bias        (100,)         100\n",
      "sequential/forward/_0/dense/kernel      (5, 100)       500\n",
      "sequential/forward/_1/dense_1/bias      (100,)         100\n",
      "sequential/forward/_1/dense_1/kernel    (100, 100)  10,000\n",
      "sequential_1/forward/_0/dense_2/bias    (100,)         100\n",
      "sequential_1/forward/_0/dense_2/kernel  (120, 100)  12,000\n",
      "sequential_1/forward/_1/dense_3/bias    (100,)         100\n",
      "sequential_1/forward/_1/dense_3/kernel  (100, 100)  10,000\n",
      "\n",
      "[Epoch 1/30, Step 100] step time: 0.009911s (±0.03617s); valid time: 0.1939s; loss: 90.1273 (±40.5918); valid loss: 58.1804 (*)\n",
      "[Epoch 2/30, Step 200, ETA 49.19s] step time: 0.005922s (±0.01229s); valid time: 0.1152s; loss: 47.9205 (±8.97867); valid loss: 41.1912 (*)\n",
      "WARNING:tensorflow:From /root/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "[Epoch 2/30, Step 300, ETA 44.5s] step time: 0.006342s (±0.01636s); valid time: 0.1583s; loss: 34.032 (±6.86303); valid loss: 27.4335 (*)\n",
      "[Epoch 3/30, Step 400, ETA 41.9s] step time: 0.005914s (±0.01222s); valid time: 0.1134s; loss: 22.0508 (±6.32486); valid loss: 16.8982 (*)\n",
      "[Epoch 3/30, Step 500, ETA 39.38s] step time: 0.005744s (±0.01196s); valid time: 0.116s; loss: 13.211 (±4.36719); valid loss: 13.9808 (*)\n",
      "[Epoch 4/30, Step 600, ETA 38.07s] step time: 0.005944s (±0.01233s); valid time: 0.1147s; loss: 9.00922 (±4.62372); valid loss: 10.6065 (*)\n",
      "[Epoch 4/30, Step 700, ETA 36.54s] step time: 0.005834s (±0.01195s); valid time: 0.1131s; loss: 6.20257 (±4.55053); valid loss: 9.53089 (*)\n",
      "[Epoch 5/30, Step 800, ETA 35.5s] step time: 0.005765s (±0.01308s); valid time: 0.1261s; loss: 3.99192 (±4.02236); valid loss: 8.28164 (*)\n",
      "[Epoch 5/30, Step 900, ETA 33.93s] step time: 0.005163s (±0.00681s); valid time: 0.05764s; loss: 3.29487 (±4.41678); valid loss: 8.79381\n",
      "[Epoch 6/30, Step 1000, ETA 32.79s] step time: 0.004961s (±0.007066s); valid time: 0.06501s; loss: 0.76964 (±4.26757); valid loss: 11.2488\n",
      "[Epoch 6/30, Step 1100, ETA 32.09s] step time: 0.006357s (±0.01418s); valid time: 0.1342s; loss: -0.451942 (±3.64402); valid loss: 7.94077 (*)\n",
      "[Epoch 7/30, Step 1200, ETA 31.58s] step time: 0.006345s (±0.01388s); valid time: 0.1274s; loss: -1.01676 (±4.48098); valid loss: 4.27248 (*)\n",
      "[Epoch 7/30, Step 1300, ETA 30.44s] step time: 0.005199s (±0.00679s); valid time: 0.0595s; loss: -1.50567 (±3.82507); valid loss: 4.96376\n",
      "[Epoch 8/30, Step 1400, ETA 29.57s] step time: 0.005254s (±0.008263s); valid time: 0.07777s; loss: -2.56848 (±3.82246); valid loss: 5.93558\n",
      "[Epoch 8/30, Step 1500, ETA 28.63s] step time: 0.005396s (±0.007601s); valid time: 0.065s; loss: -3.7328 (±3.99493); valid loss: 4.59028\n",
      "[Epoch 9/30, Step 1600, ETA 27.84s] step time: 0.00525s (±0.006712s); valid time: 0.05739s; loss: -4.54345 (±4.0636); valid loss: 4.54679\n",
      "[Epoch 9/30, Step 1700, ETA 26.99s] step time: 0.005448s (±0.007194s); valid time: 0.05858s; loss: -3.64111 (±4.65519); valid loss: 5.14178\n",
      "[Epoch 10/30, Step 1800, ETA 26.31s] step time: 0.005561s (±0.01387s); valid time: 0.1375s; loss: -5.16195 (±3.80728); valid loss: 3.98985 (*)\n",
      "[Epoch 10/30, Step 1900, ETA 25.73s] step time: 0.006493s (±0.01546s); valid time: 0.1481s; loss: -6.02573 (±4.14861); valid loss: 3.41767 (*)\n",
      "[Epoch 10/30, Step 1960, ETA 25.14s] Learning rate decreased to 0.00075\n",
      "[Epoch 11/30, Step 2000, ETA 25.14s] step time: 0.005779s (±0.01282s); valid time: 0.1223s; loss: -5.88254 (±3.90532); valid loss: 2.91247 (*)\n",
      "[Epoch 11/30, Step 2100, ETA 24.31s] step time: 0.005196s (±0.006978s); valid time: 0.06052s; loss: -7.32491 (±4.14118); valid loss: 3.59438\n",
      "[Epoch 12/30, Step 2200, ETA 23.66s] step time: 0.005595s (±0.01193s); valid time: 0.1149s; loss: -7.74021 (±3.66227); valid loss: 1.17805 (*)\n",
      "[Epoch 12/30, Step 2300, ETA 22.86s] step time: 0.005211s (±0.006955s); valid time: 0.05878s; loss: -8.42229 (±4.56481); valid loss: 1.4772\n",
      "[Epoch 13/30, Step 2400, ETA 22.18s] step time: 0.005327s (±0.00726s); valid time: 0.05717s; loss: -7.65425 (±4.09744); valid loss: 1.2484\n",
      "[Epoch 13/30, Step 2500, ETA 21.46s] step time: 0.005564s (±0.007711s); valid time: 0.06036s; loss: -8.93289 (±3.57849); valid loss: 1.23276\n",
      "[Epoch 14/30, Step 2600, ETA 20.77s] step time: 0.005139s (±0.006868s); valid time: 0.05673s; loss: -7.87832 (±3.54913); valid loss: 4.26326\n",
      "[Epoch 14/30, Step 2700, ETA 20.13s] step time: 0.00597s (±0.01309s); valid time: 0.1261s; loss: -8.2549 (±4.10663); valid loss: 1.06998 (*)\n",
      "[Epoch 15/30, Step 2800, ETA 19.47s] step time: 0.005323s (±0.007131s); valid time: 0.06031s; loss: -8.81557 (±4.04211); valid loss: 1.43518\n",
      "[Epoch 15/30, Step 2900, ETA 18.84s] step time: 0.006068s (±0.01244s); valid time: 0.1159s; loss: -8.46658 (±4.01006); valid loss: 0.350039 (*)\n",
      "[Epoch 16/30, Step 3000, ETA 18.15s] step time: 0.004839s (±0.006441s); valid time: 0.05738s; loss: -9.57815 (±3.55775); valid loss: 1.5148\n",
      "[Epoch 16/30, Step 3100, ETA 17.47s] step time: 0.005536s (±0.008078s); valid time: 0.0672s; loss: -9.43352 (±3.79976); valid loss: 0.569361\n",
      "[Epoch 17/30, Step 3200, ETA 16.82s] step time: 0.005205s (±0.006817s); valid time: 0.0571s; loss: -9.32893 (±3.81866); valid loss: 1.22345\n",
      "[Epoch 17/30, Step 3300, ETA 16.21s] step time: 0.006229s (±0.01276s); valid time: 0.1196s; loss: -9.98557 (±4.78548); valid loss: -0.186071 (*)\n",
      "[Epoch 18/30, Step 3400, ETA 15.62s] step time: 0.006058s (±0.009915s); valid time: 0.07408s; loss: -10.5223 (±4.10939); valid loss: 1.01628\n",
      "[Epoch 18/30, Step 3500, ETA 14.95s] step time: 0.00543s (±0.006987s); valid time: 0.05753s; loss: -11.0106 (±4.55315); valid loss: 2.37196\n",
      "[Epoch 19/30, Step 3600, ETA 14.33s] step time: 0.005569s (±0.008231s); valid time: 0.07182s; loss: -10.767 (±3.97772); valid loss: 0.206959\n",
      "[Epoch 19/30, Step 3700, ETA 13.67s] step time: 0.005543s (±0.007516s); valid time: 0.06723s; loss: -10.7256 (±4.37228); valid loss: 0.535648\n",
      "[Epoch 20/30, Step 3800, ETA 13.01s] step time: 0.004803s (±0.007307s); valid time: 0.0584s; loss: -10.4425 (±4.06214); valid loss: 0.456828\n",
      "[Epoch 20/30, Step 3900, ETA 12.4s] step time: 0.006352s (±0.01585s); valid time: 0.1518s; loss: -11.8969 (±4.77868); valid loss: -0.843708 (*)\n",
      "[Epoch 20/30, Step 3920, ETA 12.26s] Learning rate decreased to 0.0005625000000000001\n",
      "[Epoch 21/30, Step 4000, ETA 11.8s] step time: 0.005997s (±0.008769s); valid time: 0.05909s; loss: -11.6878 (±4.39521); valid loss: -0.21485\n",
      "[Epoch 21/30, Step 4100, ETA 11.15s] step time: 0.005576s (±0.007464s); valid time: 0.05829s; loss: -11.128 (±4.59185); valid loss: 1.38411\n",
      "[Epoch 22/30, Step 4200, ETA 10.51s] step time: 0.005095s (±0.008727s); valid time: 0.06572s; loss: -11.1878 (±4.03159); valid loss: -0.0489409\n",
      "[Epoch 22/30, Step 4300, ETA 9.857s] step time: 0.005353s (±0.007776s); valid time: 0.06626s; loss: -13.0458 (±4.9097); valid loss: -0.198513\n",
      "[Epoch 23/30, Step 4400, ETA 9.218s] step time: 0.005071s (±0.006867s); valid time: 0.0564s; loss: -12.3423 (±4.0462); valid loss: -0.424078\n",
      "[Epoch 23/30, Step 4500, ETA 8.578s] step time: 0.005431s (±0.007048s); valid time: 0.05435s; loss: -12.4888 (±4.70158); valid loss: 0.270348\n",
      "[Epoch 24/30, Step 4600, ETA 7.948s] step time: 0.005155s (±0.008186s); valid time: 0.05947s; loss: -12.6065 (±4.52886); valid loss: -0.622597\n",
      "[Epoch 24/30, Step 4700, ETA 7.326s] step time: 0.005873s (±0.01307s); valid time: 0.1254s; loss: -12.1567 (±4.03201); valid loss: -1.19023 (*)\n",
      "[Epoch 25/30, Step 4800, ETA 6.722s] step time: 0.006217s (±0.01371s); valid time: 0.1292s; loss: -13.805 (±4.75456); valid loss: -1.54539 (*)\n",
      "[Epoch 25/30, Step 4900, ETA 6.116s] step time: 0.006793s (±0.01433s); valid time: 0.1233s; loss: -12.935 (±4.15676); valid loss: -1.65634 (*)\n",
      "[Epoch 26/30, Step 5000, ETA 5.486s] step time: 0.005166s (±0.006848s); valid time: 0.05979s; loss: -13.2299 (±4.54368); valid loss: -0.286276\n",
      "[Epoch 27/30, Step 5100, ETA 4.861s] step time: 0.005395s (±0.006998s); valid time: 0.0589s; loss: -13.0528 (±3.9994); valid loss: -1.08011\n",
      "[Epoch 27/30, Step 5200, ETA 4.24s] step time: 0.00611s (±0.01249s); valid time: 0.117s; loss: -13.2974 (±4.35661); valid loss: -1.77085 (*)\n",
      "[Epoch 28/30, Step 5300, ETA 3.613s] step time: 0.00513s (±0.0071s); valid time: 0.06031s; loss: -14.0425 (±3.9945); valid loss: -1.6685\n",
      "[Epoch 28/30, Step 5400, ETA 2.984s] step time: 0.0053s (±0.007109s); valid time: 0.05833s; loss: -13.3326 (±3.91137); valid loss: -1.14765\n",
      "[Epoch 29/30, Step 5500, ETA 2.364s] step time: 0.005719s (±0.00845s); valid time: 0.05969s; loss: -13.4004 (±4.25457); valid loss: 1.45433\n",
      "[Epoch 29/30, Step 5600, ETA 1.743s] step time: 0.006199s (±0.01272s); valid time: 0.1195s; loss: -13.2713 (±3.99227); valid loss: -1.8164 (*)\n",
      "[Epoch 30/30, Step 5700, ETA 1.12s] step time: 0.005174s (±0.007527s); valid time: 0.06437s; loss: -14.2614 (±4.47545); valid loss: -1.70687\n",
      "[Epoch 30/30, Step 5800, ETA 0.4974s] step time: 0.005781s (±0.01252s); valid time: 0.1202s; loss: -13.8435 (±4.03562); valid loss: -1.99013 (*)\n",
      "[Epoch 30/30, Step 5880, ETA 0s] Learning rate decreased to 0.00042187500000000005\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmp5pwrisfi/variables.dat-5800\n",
      "INFO:tensorflow:Restoring parameters from /root/autodl-tmp/model/KPI/variables.dat\n",
      "WARNING:tensorflow:From /root/autodl-tmp/donut/reconstruction.py:53: The name tf.assert_equal is deprecated. Please use tf.compat.v1.assert_equal instead.\n",
      "\n",
      "Number of predictions: 30791\n",
      "--  final results --\n",
      "Best anomaly threshold 28.20000000000002\n",
      "Anomalies found: 1034/1023\n",
      "--- normal rows ---\n",
      "precision: 1.000\n",
      "recall: 1.000\n",
      "fscore: 1.000\n",
      "--- anomaly rows ---\n",
      "precision: 0.989\n",
      "recall: 1.000\n",
      "fscore: 0.995\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from donut import complete_timestamp, standardize_kpi, Donut, DonutTrainer, DonutPredictor\n",
    "from tensorflow import keras as K\n",
    "from tfsnippet.modules import Sequential\n",
    "from tfsnippet.utils import get_variables_as_dict, VariableSaver\n",
    "import tensorflow.compat.v1 as tf\n",
    "import os\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import f1_score\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "# path to the dataset\n",
    "file_csv = \"KPI.csv\"\n",
    "\n",
    "# Read the raw data.\n",
    "data = pd.read_csv(file_csv)\n",
    "timestamp = data[\"timestamp\"]\n",
    "values = data[\"value\"]\n",
    "labels = data[\"label\"]\n",
    "dataset_name = file_csv.split('.')[0]\n",
    "print(\"Timestamps: {}\".format(timestamp.shape[0]))\n",
    "\n",
    "# Complete the timestamp filling missing points with zeros, and obtain the missing point indicators.\n",
    "timestamp, missing, (values, labels) = complete_timestamp(timestamp, (values, labels))\n",
    "print(\"Missing points: {}\".format(np.sum(missing == 1)))\n",
    "print(\"Labeled anomalies: {}\".format(np.sum(labels == 1)))\n",
    "\n",
    "# Split the training and testing data.\n",
    "test_portion = 0.3\n",
    "test_n = int(len(values) * test_portion)\n",
    "train_values, test_values = values[:-test_n], values[-test_n:]\n",
    "train_labels, test_labels = labels[:-test_n], labels[-test_n:]\n",
    "train_missing, test_missing = missing[:-test_n], missing[-test_n:]\n",
    "print(\"Rows in test set: {}\".format(test_values.shape[0]))\n",
    "print(\"Anomalies in test set: {}\".format(np.sum(test_labels == 1)))\n",
    "\n",
    "# Standardize the training and testing data, anomaly points or missing points are excluded\n",
    "train_values, mean, std = standardize_kpi(\n",
    "    train_values, excludes=np.logical_or(train_labels, train_missing))\n",
    "test_values, _, _ = standardize_kpi(test_values, mean=mean, std=std)\n",
    "print(\"Train values mean: {}\".format(mean))\n",
    "print(\"Train values std: {}\".format(std))\n",
    "\n",
    "sliding_window = 120\n",
    "\n",
    "# define the model inside the 'model_vs' scope\n",
    "with tf.variable_scope('model') as model_vs:\n",
    "    model = Donut(\n",
    "        h_for_p_x=Sequential([\n",
    "            K.layers.Dense(100, kernel_regularizer=K.regularizers.l2(0.001),\n",
    "                           activation=tf.nn.relu),\n",
    "            K.layers.Dense(100, kernel_regularizer=K.regularizers.l2(0.001),\n",
    "                           activation=tf.nn.relu),\n",
    "        ]),\n",
    "        h_for_q_z=Sequential([\n",
    "            K.layers.Dense(100, kernel_regularizer=K.regularizers.l2(0.001),\n",
    "                           activation=tf.nn.relu),\n",
    "            K.layers.Dense(100, kernel_regularizer=K.regularizers.l2(0.001),\n",
    "                           activation=tf.nn.relu),\n",
    "        ]),\n",
    "        x_dims=sliding_window,\n",
    "        z_dims=5,\n",
    "    )\n",
    "# use DonutTrainer class to train the model\n",
    "trainer = DonutTrainer(model=model, model_vs=model_vs, max_epoch=30)\n",
    "# use DonutPredictor class to make predictions\n",
    "predictor = DonutPredictor(model)\n",
    "\n",
    "save_dir = \"model/\" + dataset_name + \"/\"\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "saved = True\n",
    "if len(os.listdir(save_dir)) == 0:\n",
    "    saved = False\n",
    "\n",
    "if saved is False:\n",
    "    with tf.Session().as_default():\n",
    "        # train the model\n",
    "        trainer.fit(train_values, train_labels, train_missing, mean, std)\n",
    "        # save variables to 'save_dir' directory\n",
    "        var_dict = get_variables_as_dict(model_vs)\n",
    "        saver = VariableSaver(var_dict, save_dir)\n",
    "        saver.save()\n",
    "        saved = True\n",
    "if saved:\n",
    "    with tf.Session().as_default():\n",
    "        # restore variables from 'save_dir'\n",
    "        saver = VariableSaver(get_variables_as_dict(model_vs), save_dir)\n",
    "        saver.restore()\n",
    "        # make predictions\n",
    "        test_score = predictor.get_score(test_values, test_missing)\n",
    "        print(\"Number of predictions: {}\".format(test_score.shape[0]))\n",
    "        # try different thresholds\n",
    "        best_threshold = 0\n",
    "        best_f1 = 0\n",
    "        best_predictions = []\n",
    "        thresholds = np.arange(5, 50, 0.2)\n",
    "        for t in thresholds:\n",
    "            threshold = t  # can be changed to better fit the training data\n",
    "            anomaly_predictions = []\n",
    "            for l in test_score:\n",
    "                if abs(l) > threshold:\n",
    "                    anomaly_predictions.append(1)\n",
    "                else:\n",
    "                    anomaly_predictions.append(0)\n",
    "            # strategy to compute modified metrics\n",
    "            # https://arxiv.org/pdf/1802.03903.pdf, fig 7\n",
    "            for i in range(sliding_window-1, len(anomaly_predictions)):\n",
    "                if anomaly_predictions[i-sliding_window+1] == 1 and test_labels[i] == 1:  # true positive\n",
    "                    j = i-1\n",
    "                    while j >= sliding_window-1 and test_labels[j] == 1\\\n",
    "                            and anomaly_predictions[j-sliding_window+1] == 0:\n",
    "                        anomaly_predictions[j-sliding_window+1] = 1\n",
    "                        j -= 1\n",
    "                    j = i+1\n",
    "                    while j < len(anomaly_predictions) and test_labels[j] == 1\\\n",
    "                            and anomaly_predictions[j-sliding_window+1] == 0:\n",
    "                        anomaly_predictions[j-sliding_window+1] = 1\n",
    "                        j += 1\n",
    "            f1 = f1_score(test_labels[sliding_window-1:], anomaly_predictions, average='binary')\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_threshold = threshold\n",
    "                best_predictions = anomaly_predictions\n",
    "\n",
    "        anomaly_predictions = np.array(best_predictions)\n",
    "        print(\"--  final results --\")\n",
    "        print(\"Best anomaly threshold {}\".format(best_threshold))\n",
    "        print(\"Anomalies found: {}/{}\".format(np.sum(anomaly_predictions == 1), np.sum(test_labels == 1)))\n",
    "        prfs = precision_recall_fscore_support(test_labels[sliding_window-1:], anomaly_predictions)\n",
    "        print(\"--- normal rows ---\")\n",
    "        print(\"precision: {:.3f}\".format(prfs[0][0]))\n",
    "        print(\"recall: {:.3f}\".format(prfs[1][0]))\n",
    "        print(\"fscore: {:.3f}\".format(prfs[2][0]))\n",
    "        print(\"--- anomaly rows ---\")\n",
    "        print(\"precision: {:.3f}\".format(prfs[0][1]))\n",
    "        print(\"recall: {:.3f}\".format(prfs[1][1]))\n",
    "        print(\"fscore: {:.3f}\".format(prfs[2][1]))\n",
    "\n",
    "        \"\"\"\n",
    "        --  final results (cpu4 dataset) --\n",
    "        --  final results --\n",
    "        Best anomaly threshold 7.2\n",
    "        Anomalies found: 10/14\n",
    "        --- normal rows ---\n",
    "        precision: 0.999\n",
    "        recall: 1.000\n",
    "        fscore: 1.000\n",
    "        --- anomaly rows ---\n",
    "        precision: 1.000\n",
    "        recall: 0.714\n",
    "        fscore: 0.833\n",
    "        \n",
    "        --  final results (timeseries dataset) --\n",
    "        Best anomaly threshold 46.8\n",
    "        Anomalies found: 1032/1023\n",
    "        --- normal rows ---\n",
    "        precision: 1.000\n",
    "        recall: 1.000\n",
    "        fscore: 1.000\n",
    "        --- anomaly rows ---\n",
    "        precision: 0.991\n",
    "        recall: 1.000\n",
    "        fscore: 0.996\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8847a6fc-aefb-4439-9f24-03f9759e1bf7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
